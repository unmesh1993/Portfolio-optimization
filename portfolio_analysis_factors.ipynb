{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNQrfCcSaBpAtnPheG3rxq1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"rpAoDCtpJdI_","executionInfo":{"status":"ok","timestamp":1753699910719,"user_tz":-330,"elapsed":32246,"user":{"displayName":"Unmesh Mondal","userId":"16495294591831246928"}}},"outputs":[],"source":["from datetime import date\n","import random\n","import time\n","import yfinance as yf\n","import pandas as pd\n","\n","import seaborn as sns\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","\n","from numpy.fft import fft, ifft, fftshift\n","import numpy as np\n","from numpy import log, sqrt, exp\n","\n","\n","from sklearn.linear_model import Ridge\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.mixture import GaussianMixture\n","from sklearn.linear_model import LinearRegression\n","\n","from statsmodels.tsa.statespace.sarimax import SARIMAX\n","from statsmodels.stats.diagnostic import acorr_ljungbox\n","\n","import scipy.stats as stats\n","from scipy.stats import probplot, laplace, norm, t, poisson\n","from scipy.linalg import solve_banded\n","from scipy.optimize import minimize, differential_evolution\n","from scipy.integrate import quad\n","from scipy.special import roots_laguerre\n","from scipy.interpolate import interp1d\n","from scipy.sparse import diags, kron, identity, csr_matrix\n","from scipy.sparse.linalg import spsolve\n","from scipy.stats import multivariate_normal, kstest\n","\n","import statsmodels.api as sm\n","from statsmodels.nonparametric.kde import KDEUnivariate\n","from statsmodels.tsa.stattools import adfuller, kpss\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","from statsmodels.tsa.statespace.sarimax import SARIMAX\n","from statsmodels.tsa.arima_process import ArmaProcess\n","\n","import pymc as pm\n","import arviz as az\n","\n","#import aesara.tensor as at\n","\n","from tensorflow import keras\n","#from tensorflow.keras.utils import plot_model\n","\n","#import pyswarms as ps\n","\n","######################################\n","#from pmdarima import auto_arima\n","#from diptest import diptest\n","\n"]},{"cell_type":"code","source":["class FactorModels:\n","    def __init__(self, rfr, log_return_pf, log_return_index):\n","        \"\"\"\n","        Parameters:\n","        - rfr: scalar, annualized or periodic risk-free rate\n","        - log_return_index: Series, market index log returns\n","        - log_return_pf: DataFrame, asset log returns (columns = assets)\n","        \"\"\"\n","        self.rfr = rfr\n","        self.trading_days = 252  #\n","        self.lam = 2  #  # placeholder for other models like Black-Litterman\n","        self.fit_type = None\n","\n","        log_return_pf, log_return_index = self.clean_df(log_return_pf, log_return_index)\n","        self.log_return_pf =  log_return_pf\n","        self.log_return_index = log_return_index\n","\n","    def clean_df(self, df1, df2):\n","        # Assume df1 and df2 are your time-indexed DataFrames\n","        # Ensure the index is datetime\n","        df1.index = pd.to_datetime(df1.index)\n","        df2.index = pd.to_datetime(df2.index)\n","\n","        # Step 1: Align on common time index (inner join on index)\n","        common_index = df1.index.intersection(df2.index)\n","        df1_common = df1.loc[common_index]\n","        df2_common = df2.loc[common_index]\n","\n","        # Step 2: Concatenate to identify rows with any NaNs across both\n","        combined = pd.concat([df1_common, df2_common], axis=1)\n","\n","        # Step 3: Drop any rows with NaNs across both\n","        combined_clean = combined.dropna()\n","\n","        # Step 4: Split back into two DataFrames with identical indices\n","        n1 = df1.shape[1]\n","        df1_clean = combined_clean.iloc[:, :n1]\n","        df2_clean = combined_clean.iloc[:, n1:]\n","\n","        # Optional: verify they have the same index\n","        assert all(df1_clean.index == df2_clean.index)\n","\n","        return df1_clean, df2_clean\n","#########################################################################################\n","\n","    def CAPM_OLS(self, fit_type):\n","        \"\"\"\n","        Estimate CAPM expected returns using OLS regression and print summary for each asset.\n","\n","        Returns:\n","        - DataFrame with beta, alpha, CAPM expected return\n","        - Prints statsmodels summary for each regression\n","        \"\"\"\n","        self.fit_type = fit_type\n","        mu_index = self.fit_marginal(self.log_return_index)[0]\n","        mu_pf = self.fit_marginal(self.log_return_pf)[0]\n","\n","        market_expected_return = mu_index[0]\n","        results = {}\n","\n","        X = self.log_return_index.values\n","        X = sm.add_constant(X)  # Adds intercept term\n","\n","        for asset in self.log_return_pf.columns:\n","            Y = self.log_return_pf[asset].values\n","            model = sm.OLS(Y, X)\n","            res = model.fit()\n","\n","            alpha = res.params[0]\n","            beta = res.params[1]\n","            capm_return = self.rfr + beta * (market_expected_return - self.rfr)\n","\n","            print(f\"--- {asset} ---\")\n","            print(res.summary())\n","\n","            results[asset] = {\n","                'alpha': alpha,\n","                'beta': beta,\n","                'CAPM_return': capm_return\n","            }\n","\n","        return pd.DataFrame(results).T\n","\n","\n","    def CAPM_GLS(self, fit_type):\n","        \"\"\"\n","        Estimate CAPM expected returns using GLS regression and print summary for each asset.\n","\n","        Returns:\n","        - DataFrame with beta, alpha, and CAPM expected return for each asset\n","        - Prints statsmodels GLS summary for each regression\n","        \"\"\"\n","        self.fit_type = fit_type\n","        mu_index = self.fit_marginal(self.log_return_index)[0]\n","        mu_pf = self.fit_marginal(self.log_return_pf)[0]\n","\n","        market_expected_return = mu_index[0]\n","        results = {}\n","\n","        X = self.log_return_index.values\n","        X = sm.add_constant(X)  # Add intercept\n","\n","        for asset in self.log_return_pf.columns:\n","            Y = self.log_return_pf[asset].values\n","\n","            # Estimate weights (variance of residuals from OLS as a proxy)\n","            ols_model = sm.OLS(Y, X).fit()\n","            resid_var = np.var(ols_model.resid)\n","            weights = 1 / resid_var  # Inverse-variance weights (constant here but placeholder for real GLS)\n","\n","            # In general GLS, you'd use a full covariance matrix or per-observation variance\n","            gls_model = sm.GLS(Y, X, sigma=np.eye(len(Y)) * resid_var).fit()\n","\n","            alpha = gls_model.params[0]\n","            beta = gls_model.params[1]\n","            capm_return = self.rfr + beta * (market_expected_return - self.rfr)\n","\n","            print(f\"--- GLS Summary for {asset} ---\")\n","            print(gls_model.summary())\n","\n","            results[asset] = {\n","                'alpha': alpha,\n","                'beta': beta,\n","                'CAPM_return': capm_return\n","            }\n","\n","        return pd.DataFrame(results).T\n","##########################################################################################\n","    def log_return_val(self, pf):\n","        \"\"\"\n","        Compute log returns for each stock (ignoring NaNs),\n","        fit normal and t-distributions, and plot boxplots and KS stats.\n","        \"\"\"\n","        log_returns = pd.DataFrame()\n","\n","        for col in self.pf.columns:\n","            series = pd.to_numeric(pf[col], errors='coerce')\n","            first_valid = series[series > 0].first_valid_index()\n","            if first_valid is not None:\n","                trimmed = series.loc[first_valid:]\n","                log_ret = np.log(trimmed / trimmed.shift(1)).dropna()\n","                log_returns[col] = log_ret\n","\n","        return log_returns\n","\n","    def fit_marginal(self, log_returns):\n","        \"\"\"\n","        Marginal (univariate) fit of log_returns based on self.fit_type ('norm' or 't').\n","        Returns:\n","            mu_vector: daily means,\n","            cov_matrix: daily diagonal covariance matrix,\n","            ks_stats: KS statistics per asset\n","        \"\"\"\n","        mu_vector = []\n","        std_vector = []\n","        ks_stats = {}\n","\n","        for col in log_returns.columns:\n","            data = log_returns[col].dropna()\n","\n","            if self.fit_type == \"norm\":\n","                mu, std = stats.norm.fit(data)\n","                ks = stats.kstest(data, 'norm', args=(mu, std)).statistic\n","                mu_vector.append(mu)\n","                std_vector.append(std)\n","                ks_stats[col] = ks\n","\n","            elif self.fit_type == \"t\":\n","                df_t, loc_t, scale_t = stats.t.fit(data)\n","                ks = stats.kstest(data, 't', args=(df_t, loc_t, scale_t)).statistic\n","                mu_vector.append(loc_t)\n","                std = scale_t * np.sqrt(df_t / (df_t - 2)) if df_t > 2 else np.nan\n","                std_vector.append(std)\n","                ks_stats[col] = ks\n","\n","            else:\n","                raise ValueError(\"fit_type must be 'norm' or 't'\")\n","\n","        # Convert to numpy arrays\n","        mu_vector = np.array(mu_vector)\n","        std_vector = np.array(std_vector)\n","\n","        mu_annual = mu_vector * self.trading_days\n","        std_annual = std_vector * np.sqrt(self.trading_days)\n","\n","        if self.fit_type == 'norm':\n","            args = mu_annual, std_annual\n","        elif self.fit_type == 't':\n","            args = mu_annual, std_annual, df_t\n","\n","        #self.plot_log_return(list(log_returns.columns), list(ks_stats.values()), mu_annual, np.sqrt(np.diag(cov_annual)), corr_matrix)\n","\n","        return args\n","\n","    def plot_log_return(self, asset_names, ks_values, mu_annual, sigma_annual, log_corr):\n","\n","        # Create subplots\n","        fig, axes = plt.subplots(3, 1, figsize=(14, 16))\n","\n","        # --- Subplot 1: Annualized Mean ± 1 Std Dev ---\n","        axes[0].errorbar(asset_names, mu_annual*100, yerr=sigma_annual*100, fmt='o', capsize=5, color='dodgerblue', label=\"Annual Mean ± 1 Std\")\n","        axes[0].set_title(f\"Annualized Mean ± 1 Std Dev ({self.fit_type})\")\n","        axes[0].set_ylabel(\"Annual Return\")\n","        axes[0].grid(True)\n","        axes[0].legend()\n","\n","        # --- Subplot 2: KS Statistic ---\n","        axes[1].bar(asset_names, ks_values, color='mediumpurple')\n","        axes[1].axhline(0.05, color='green', linestyle='--', label='Excellent Fit (<0.05)')\n","        axes[1].axhline(0.10, color='orange', linestyle='--', label='Acceptable Fit (<0.10)')\n","        axes[1].set_title(f\"Marginal KS Statistics ({self.fit_type})\")\n","        axes[1].set_ylabel(\"KS Statistic\")\n","        axes[1].legend()\n","        axes[1].grid(True)\n","\n","        # --- Subplot 3: Correlation Heatmap ---\n","        sns.heatmap(log_corr, ax=axes[2], cmap=\"coolwarm\", annot=True, fmt=\".2f\", center=0,\n","                    xticklabels=asset_names, yticklabels=asset_names)\n","        axes[2].set_title(\"Pearson Correlation Matrix of Log Returns\")\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","\n"],"metadata":{"id":"mAIRcaR5Jyq7","executionInfo":{"status":"ok","timestamp":1753699910759,"user_tz":-330,"elapsed":26,"user":{"displayName":"Unmesh Mondal","userId":"16495294591831246928"}}},"execution_count":2,"outputs":[]}]}